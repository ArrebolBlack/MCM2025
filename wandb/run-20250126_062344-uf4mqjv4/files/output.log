Epoch 1/10:   0%|          | 0/2388 [00:00<?, ?batch/s]
Traceback (most recent call last):
  File "E:\25MCM\q1_2_main.py", line 36, in <module>
    train(model, train_loader, criterion, optimizer, num_epochs=10, device='cuda' if torch.cuda.is_available() else 'cpu', save_path="checkpoints/", scheduler=scheduler)
  File "E:\25MCM\utils.py", line 37, in train
    loss.backward()  # 反向传播
    ^^^^^^^^^^^^^^^
  File "D:\anaconda\envs\Project1\Lib\site-packages\torch\_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "D:\anaconda\envs\Project1\Lib\site-packages\torch\autograd\__init__.py", line 267, in backward
    _engine_run_backward(
  File "D:\anaconda\envs\Project1\Lib\site-packages\torch\autograd\graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Found dtype Long but expected Float
tensor([[86.7552, 23.3949, 11.5973,  8.2526]])
输出形状: torch.Size([1, 2])